import time
from app.engines import db
from app.models import Domain

from app.logger import ContextLogger
from app.tasks.threat.task import Task
from bs4 import BeautifulSoup


class CrawlMalwaredomainlis(Task):
    def __init__(self):
        super().__init__('malwaredomainlis 数据爬取')
        self.logger = ContextLogger('threat_domain')

    def run_crawl(self):
        start = time.time()
        domains = []
        source = 'www.malwaredomainlist.com'
        for i in range(24):
            url = 'http://www.malwaredomainlist.com/mdl.php?' \
                  'inactive=&sort=Date&search=&colsearch=All&' \
                  'ascordesc=DESC&quantity=100&page=%s' % i
            _info = self.get(url=url)
            if _info is None:
                self.logger.warning("request returned None   " + source)
                return None
            html = _info.replace('<wbr>', '')
            soup = BeautifulSoup(html, 'lxml')
            tables = soup.findChildren('table')
            table = tables[1]
            rows = table.findChildren('tr', attrs={'class': None})
            for row in rows:
                dt = row.findChildren('td')[0].string
                tma = time.strptime(dt, "%Y/%m/%d_%H:%M")
                updatetime = time.strftime("%Y-%m-%d", tma)
                domain = row.findChildren('td')[1].string
                ip = row.findChildren('td')[2].string
                description = row.findChildren('td')[4].string
                registrant = row.findChildren('td')[5].string
                ASN = row.findChildren('td')[6].string
                reverseLookup = row.findChildren('td')[3].string
                if domain != '-':
                    domain = domain.split('/')
                    domain = domain.pop(0)
                elif domain == '-':
                    ip = ip.split('/')
                    ip = ip.pop(0)
                domains.append([domain, ip, updatetime, source, description, registrant, ASN, reverseLookup])
        stop = time.time()
        crawl_time = str(stop - start) + "秒"
        self.save_info(domains, source, crawl_time)

    def save_info(self, domains, source, crawl_time):

        start = time.time()
        all_count = len(domains)
        avail_count = 0

        if len(domains) > 0:
            try:
                for domain, ip, updatetime, source, description, registrant, ASN, reverseLookup in domains:
                        flag = db.session.query(Domain).filter(Domain.domain == domain, Domain.source == source).first()

                        if flag is None:
                            new_domain = Domain()
                            new_domain.domain = domain
                            new_domain.ip = ip
                            new_domain.updatetime = updatetime
                            new_domain.source = source
                            new_domain.description = description
                            new_domain.registrant = registrant
                            new_domain.asn = ASN
                            new_domain.reverselookup = reverseLookup
                            db.session.add(new_domain)
                            avail_count += 1
                        else:
                            flag.updatetime = updatetime
                            db.session.add(flag)
                db.session.commit()
            except Exception as e:
                self.logger.warning("Error writing to database" + str(e) + source)
        else:
            self.logger.warning("NO record found from: %s" % source)
        stop = time.time()
        storage_time = str(stop - start) + "秒"

        self.logger.info("malwaredomainlist 共收集{0}条数据， 新数据{1}条".format(all_count, avail_count))
        self.logger.info("malwaredomainlist 抓取时间{0}，数据遍历时间{1}".format(crawl_time, storage_time))


if __name__ == "__main__":
    freebuf = CrawlMalwaredomainlis()
    freebuf.run_crawl()

